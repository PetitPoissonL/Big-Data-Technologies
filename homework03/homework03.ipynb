{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# New York taxis trips\n",
    "\n",
    "This homework is about New York taxi trips. Here is something from [Todd Schneider](https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/):\n",
    "\n",
    "> The New York City Taxi & Limousine Commission has released a  detailed historical dataset covering over 1 billion individual taxi trips in the city from January 2009 through December 2019. \n",
    "Taken as a whole, the detailed trip-level data is more than just a vast list of taxi pickup and drop off coordinates: it's a story of a City. \n",
    "How bad is the rush hour traffic from Midtown to JFK? \n",
    "Where does the Bridge and Tunnel crowd hang out on Saturday nights?\n",
    "What time do investment bankers get to work? How has Uber changed the landscape for taxis?\n",
    "The dataset addresses all of these questions and many more.\n",
    "\n",
    "The NY taxi trips dataset has been plowed by series of distinguished data scientists.\n",
    "The dataset is available from on Amazon S3 (Amazon's cloud storage service).\n",
    "The link for each file has the following form:\n",
    "\n",
    "    https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_{year}-{month}.csv\n",
    "\n",
    "There is one CSV file for each NY taxi service (`yellow`, `green`, `fhv`) and each calendar month (replacing `{year}` and `{month}` by the desired ones).\n",
    "Each file is moderately large, a few gigabytes. \n",
    "The full dataset is relatively large if it has to be handled on a laptop (several hundred gigabytes).\n",
    "\n",
    "You will focus on the `yellow` taxi service and a pair of months, from year 2015 and from year 2018. \n",
    "Between those two years, for hire vehicles services have taken off and carved a huge marketshare.\n",
    "\n",
    "Whatever the framework you use, `CSV` files prove hard to handle. \n",
    "After downloading the appropriate files (this takes time, but this is routine), a first step will consist in converting the csv files into a more Spark friendly format such as `parquet`.\n",
    "\n",
    "Saving into one of those formats require decisions about bucketing, partitioning and so on. Such decisions influence performance. It is your call.\n",
    "Many people have been working on this dataset, to cite but a few:\n",
    "\n",
    "\n",
    "- [1 billion trips with a vengeance](https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/)\n",
    "- [1 billion trips with R and SQL ](http://freerangestats.info/blog/2019/12/22/nyc-taxis-sql)\n",
    "- [1 billion trips with redshift](https://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html)\n",
    "- [nyc-taxi](https://github.com/fmaletski/nyc-taxi-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your internet connection, **download the files** corresponding to **\"yellow\" taxis** for the years 2015 and 2018. Download **at least one month** (the same) for 2015 and 2018, if you can download all of them.\n",
    "\n",
    "**Hint.** The 12 csv for 2015 are about 23GB in total, but the corresponding parquet file, if you can create it for all 12 months, is only about 3GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **might** need the following stuff in order to work with GPS coordinates and to plot things easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T17:22:05.049404Z",
     "start_time": "2022-03-01T17:22:02.651724Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install geojson geopandas plotly geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T17:22:07.373733Z",
     "start_time": "2022-03-01T17:22:05.058044Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipyleaflet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework **we will let you decide on the tools to use** (expected for Spark) and to **find out information all by yourself** (but don't hesitate to ask questions on the `slack` channel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Loading data as parquet files\n",
    "\n",
    "We want to organize the data on a per year and per service basis. \n",
    "We want to end up with one `parquet` file for each year and each taxi service, since parquet is much better than CSV files.\n",
    "\n",
    "**Hint.** Depending on your internet connection and your laptop, you can use only the \"yellow\" service and use one month of 2015 and 2018\n",
    "\n",
    "CSV files can contain corrupted lines. You may have to work in order to perform ETL (Extract-Transform-Load) in order obtain a properly typed data frame.\n",
    "\n",
    "You are invited to proceed as follows:\n",
    "\n",
    "1. Try to read the CSV file without imposing a schema. \n",
    "1. Inspect the inferred schema. Do you agree with Spark's typing decision?\n",
    "1. Eventually correct the schema and read again the data\n",
    "1. Save the data into parquet files\n",
    "1. In the rest of your work, **you will only use the parquet files you created**, not the csv files (don't forget to choose a partitioning column and a number of partitions when creating the parquet files).\n",
    "\n",
    "**Hint.** Don't forget to ask `Spark` to use all the memory and ressources from your computer.\n",
    "\n",
    "**Hint.** Don't foreget that you should specify a partitioning column and a number of partitions when creating the parquet files.\n",
    "\n",
    "**Hint.** Note that the schemas of the 2015 and 2018 data are different...\n",
    "\n",
    "**Hint.** When working on this, ask you and answer to the following questions:\n",
    "\n",
    "1. What is the `StorageLevel` of the dataframe after reading the csv files?\n",
    "1. What is the number of partitions of the dataframe? \n",
    "1. Is it possible to tune this number at loading time? \n",
    "1. Why would we want to modify the number of partitions when creating the parquet files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T22:27:55.763683Z",
     "start_time": "2022-03-12T22:27:51.461269Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyshp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T12:52:17.121865Z",
     "start_time": "2022-03-13T12:52:11.126723Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install descartes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:12:37.380210Z",
     "start_time": "2022-03-13T19:12:34.972908Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the usual suspects\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import timeit\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import zipfile\n",
    "import shapefile\n",
    "from shapely.geometry import Polygon, Point\n",
    "from descartes.patch import PolygonPatch\n",
    "import geopandas\n",
    "import shapely.geometry\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:12:39.452981Z",
     "start_time": "2022-03-13T19:12:39.291681Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark\n",
    "from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.catalog import Catalog\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType, LongType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:12:48.111869Z",
     "start_time": "2022-03-13T19:12:41.117433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start the SparkSession\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.memory', '16g')\n",
    "conf.set('spark.driver.memory', '8g')\n",
    "conf.set(\"spark.driver.cores\",\"4\")\n",
    "conf.set(\"spark.num.executors\",\"10\")\n",
    "conf.set(\"spark.executor.cores\",\"4\")\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    .appName(\"New York taxis trips\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:12:52.020907Z",
     "start_time": "2022-03-13T19:12:52.007401Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the number of partitions\n",
    "spark.conf.set(\"spark.default.parallelism\", 150)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:12:53.287681Z",
     "start_time": "2022-03-13T19:12:53.279136Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.auto.repartition\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to read the CSV file without imposing a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T14:33:40.879705Z",
     "start_time": "2022-03-05T14:33:40.724055Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# download the data of 2015\n",
    "path2015 = Path('yellow_tripdata_2015-07.csv')\n",
    "if not path2015.exists():\n",
    "    url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-07.csv\"\n",
    "    r = requests.get(url)\n",
    "    with open(os.path.join('./', 'yellow_tripdata_2015-07.csv'), 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T14:33:42.215744Z",
     "start_time": "2022-03-05T14:33:42.209501Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the data of 2018\n",
    "path2018 = Path('yellow_tripdata_2018-07.csv')\n",
    "if not path2018.exists():\n",
    "    url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2018-07.csv\"\n",
    "    r = requests.get(url)\n",
    "    with open(os.path.join('./', 'yellow_tripdata_2018-07.csv'), 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:58:37.512660Z",
     "start_time": "2022-03-07T12:57:46.285037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from a csv file\n",
    "df_sp2015 = spark.read\\\n",
    "             .format('csv')\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .option(\"mode\", \"FAILFAST\")\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"sep\", \",\")\\\n",
    "             .load(\"yellow_tripdata_2015-07.csv\")\n",
    "\n",
    "df_sp2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:59:06.495369Z",
     "start_time": "2022-03-07T12:58:44.059032Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sp2018 = spark.read\\\n",
    "             .format('csv')\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .option(\"mode\", \"FAILFAST\")\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"sep\", \",\")\\\n",
    "             .load(\"yellow_tripdata_2018-07.csv\")\n",
    "\n",
    "df_sp2018.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the inferred schema. Do you agree with Spark's typing decision?\n",
    "\n",
    "Non, les colonnes \"tpep_pickup_datetime\" et \"tpep_dropoff_datetime\" doivent être de type date et pas de type string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eventually correct the schema and read again the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:59:11.101316Z",
     "start_time": "2022-03-07T12:59:11.026571Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sp2015 = df_sp2015\\\n",
    "            .withColumn(\"tpep_pickup_datetime\",(fn.to_timestamp(col(\"tpep_pickup_datetime\"))))\\\n",
    "            .withColumn(\"tpep_dropoff_datetime\",(fn.to_timestamp(col(\"tpep_dropoff_datetime\"))))\n",
    "\n",
    "df_sp2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T12:59:13.121373Z",
     "start_time": "2022-03-07T12:59:13.072539Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sp2018 = df_sp2018\\\n",
    "            .withColumn(\"tpep_pickup_datetime\",(fn.to_timestamp(col(\"tpep_pickup_datetime\"))))\\\n",
    "            .withColumn(\"tpep_dropoff_datetime\",(fn.to_timestamp(col(\"tpep_dropoff_datetime\"))))\n",
    "\n",
    "df_sp2018.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data into parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T13:00:52.549745Z",
     "start_time": "2022-03-07T12:59:20.300500Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sp2015.write.mode('overwrite').partitionBy('payment_type').parquet(\"yellow_tripdata_2015-07.parquet\")\n",
    "df_sp2015.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T13:01:42.240496Z",
     "start_time": "2022-03-07T13:00:56.072065Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sp2018.write.mode('overwrite').partitionBy('payment_type').parquet(\"yellow_tripdata_2018-07.parquet\")\n",
    "df_sp2018.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:13:05.781828Z",
     "start_time": "2022-03-13T19:13:01.478273Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2015 = spark.read.parquet(\"yellow_tripdata_2015-07.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:13:07.468571Z",
     "start_time": "2022-03-13T19:13:07.118060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2018 = spark.read.parquet(\"yellow_tripdata_2018-07.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:13:11.211218Z",
     "start_time": "2022-03-13T19:13:09.356411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "def cleanData(df, year, month):\n",
    "    df = df\\\n",
    "        .withColumn('year', fn.year('tpep_pickup_datetime'))\\\n",
    "        .withColumn('month', fn.month('tpep_pickup_datetime'))\\\n",
    "        .where(col('year')==year)\\\n",
    "        .where(col('month')==month)\\\n",
    "        .where(col('tpep_pickup_datetime')<=col('tpep_dropoff_datetime'))\n",
    "    return df\n",
    "\n",
    "df2018 = cleanData(df2018, '2018','7')\n",
    "df2015 = cleanData(df2015, '2015','7')\n",
    "df2018.cache()\n",
    "df2015.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:14:04.488983Z",
     "start_time": "2022-03-13T19:13:13.314124Z"
    }
   },
   "outputs": [],
   "source": [
    "df2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:14:35.427021Z",
     "start_time": "2022-03-13T19:14:08.404150Z"
    }
   },
   "outputs": [],
   "source": [
    "df2018.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le schema est différente entre 2015 et 2018, nous ajoutons de manière préemptive certaines colonnes nécessaires telles que location_id, zone, etc. à l'ensemble de données de 2015 afin de traiter les données ultérieurement. En raison de l'énorme quantité de données et de la longue durée d'exécution, nous avons placé le code de prétraitement dans le fichier 'data_op,ipynb', qui générera un nouveau fichier csv, ici nous nous référons directement au fichier parquet converti à partir du fichier csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2015_add = spark.read.parquet(\"yellow_tripdata_2015-07_add.parquet\")\n",
    "df2015_add = cleanData(df2015_add, '2015','7')\n",
    "df2015_add.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:14:39.536145Z",
     "start_time": "2022-03-13T19:14:39.509694Z"
    }
   },
   "outputs": [],
   "source": [
    "def _map_to_pandas(rdds):\n",
    "    \"\"\" Needs to be here due to pickling issues \"\"\"\n",
    "    return [pd.DataFrame(list(rdds))]\n",
    "\n",
    "def toPandas(df, n_partitions=None):\n",
    "    \"\"\"\n",
    "    Returns the contents of 'df' as a local 'pandas.DataFrame' in a speedy fashion. The DataFrame is\n",
    "    repartitioned if 'n_partitions' is passed.\n",
    "    :param df:              pyspark.sql.DataFrame\n",
    "    :param n_partitions:    int or None\n",
    "    :return:                pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if n_partitions is not None: df = df.repartition(n_partitions)\n",
    "    df_pand = df.rdd.mapPartitions(_map_to_pandas).collect()\n",
    "    df_pand = pd.concat(df_pand)\n",
    "    df_pand.columns = df.columns\n",
    "    return df_pand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Investigate (at least) one month of data in 2015\n",
    "\n",
    "From now on, you will be using **the parquet files you created for 2015**.\n",
    "\n",
    "We shall visualize several features of taxi traffic during one calendar month\n",
    "in 2015 and the same calendar month in 2018.\n",
    "\n",
    "**Hint.** In order to build appealing graphics, you may stick to `matplotlib + seaborn`, you can use also\n",
    "`plotly`, which is used a lot to build interactive graphics, but you can use whatever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following longitudes and lattitudes encompass Newark and JFK airports, Northern Manhattan and Verazzano bridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T14:26:28.795383Z",
     "start_time": "2022-03-07T14:26:28.789857Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "long_min = -74.10\n",
    "long_max = -73.70\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using these boundaries, **filter the 2015 data** (using pickup and dropoff longitude and latitude) and count the number of trips for each value of `passenger_count` and make a plot of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2015_f = df2015.filter((col(\"pickup_longitude\").between(long_min, long_max)) & (col(\"pickup_latitude\").between(lat_min, lat_max)) & (col(\"dropoff_longitude\").between(long_min, long_max)) & (col(\"dropoff_latitude\").between(lat_min, lat_max)))\n",
    "df2018_f = df2018.filter((col(\"pickup_longitude\").between(long_min, long_max)) & (col(\"pickup_latitude\").between(lat_min, lat_max)) & (col(\"dropoff_longitude\").between(long_min, long_max)) & (col(\"dropoff_latitude\").between(lat_min, lat_max)))\n",
    "df2015_add_f = df2015_add.filter((col(\"pickup_longitude\").between(long_min, long_max)) & (col(\"pickup_latitude\").between(lat_min, lat_max)) & (col(\"dropoff_longitude\").between(long_min, long_max)) & (col(\"dropoff_latitude\").between(lat_min, lat_max)))\n",
    "\n",
    "num_trips = df2015_f\\\n",
    "    .groupBy('passenger_count')\\\n",
    "    .count()\\\n",
    "    .orderBy(\"passenger_count\")\n",
    "    \n",
    "num_trips.show()\n",
    "num_trips = num_trips.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(num_trips, \n",
    "             x=\"passenger_count\", \n",
    "             y=\"count\", \n",
    "             title=\"number of trips for each value of passenger_count for July 2015\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Trips with $0$ or larger than $7$ passengers are pretty rare.\n",
    "We suspect these to be outliers. \n",
    "We need to explore these trips further in order order to understand what might be wrong\n",
    "with them\n",
    "\n",
    "1. What's special with trips with zero passengers?\n",
    "1. What's special with trips with more than $6$ passengers?\n",
    "1. What is the largest distance travelled during this month? Is it the first taxi on the moon?\n",
    "1. Plot the distribution of the `trip_distance` (using an histogram for instance) during year 2105. Focus on trips with non-zero trip distance and trip distance less than 30 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "df2015_f = df2015_f.withColumn('dayofweek',dayofweek(df2015_f['tpep_pickup_datetime']))\n",
    "df2015_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's special with trips with zero passengers?\n",
    "zero_passengers = df2015_f.filter(col(\"passenger_count\") == 0 ).select('*').toPandas()\n",
    "zero_passengers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_passengers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's special with trips with more than  6  passengers?\n",
    "sixplus_passengers = df2015_f.filter(col(\"passenger_count\") > 6 ).select('*').toPandas()\n",
    "sixplus_passengers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixplus_passengers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the largest distance travelled during this month? Is it the first taxi on the moon?\n",
    "#La distance maximale est de 1.0083357E7, ce qui s'est produit le 2015-07-31 10:35:58 et n'est clairement pas le premier TAXI du mois.\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "df_distance = df2015_f.select('trip_distance','tpep_pickup_datetime').orderBy(df2015_f['trip_distance'].desc())\n",
    "df_distance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance = df2015_f\\\n",
    "                .where('trip_distance > 0 and trip_distance < 30')\\\n",
    "                .select('trip_distance','tpep_pickup_datetime',\n",
    "                        'pickup_longitude','pickup_latitude',\n",
    "                        'dayofweek','tip_amount','total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ceil\n",
    "\n",
    "trip_distance_f = trip_distance.withColumn('trip_distance_int',ceil(trip_distance['trip_distance']))\n",
    "trip_distance_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_histogram = trip_distance_f.groupBy('trip_distance_int').count()\n",
    "distance_histogram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_histogram = distance_histogram.orderBy(distance_histogram['trip_distance_int']).select('*').toPandas()\n",
    "\n",
    "fig = px.bar(distance_histogram, \n",
    "             x=\"trip_distance_int\", \n",
    "             y=\"count\", \n",
    "             title=\"the distribution of the trip_distance\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what Spark does for these computations\n",
    "\n",
    "1. Use the `explain` method or have a look at the [Spark UI](http://localhost:4040/SQL/) to analyze the job. You should be able to assess \n",
    "    - Parsed Logical Plan\n",
    "    - Analyzed Logical Plan\n",
    "    - Optimized Logical Plan\n",
    "    - Physical Plan\n",
    "1. Do the Analyzed Logical Plan and Optimized Logical Plan differ? Spot the differences if any. How would a RDBMS proceed with such a query?\n",
    "\n",
    "The Analyzed Logical Plan and Optimized Logical Plan are two different phases of the Spark execution plan.The Optimised Logical Plan follows the Analyzed Logical Plan stage and further optimises the output plan from the Analyzed Logical Plan stage.\n",
    "The Analyzed Logical Plan phase loads the external RDD, then serialises the fields, lists the names of the mapped tables, confirms that the tables exist, and then filters by condition to get the result set.\n",
    "The Optimized Logical Plan stage optimises the filtering criteria output from the Analyzed Logical Plan stage to determine if it is null (neither hive nor relational databases have this), the difference between this and RDD is that rdd loads all the data in, whereas sparksql stops directly if it encounters a null value.\n",
    "\n",
    "\n",
    "1. How does the physical plan differ from the Optimized Logical Plan? What are the keywords you would not expects in a RDBMS? What is their meaning?\n",
    "\n",
    "the physical plan belongs to the physical execution plan phase, which converts the upstream logical execution plan into a physical execution plan.\n",
    "\n",
    "\n",
    "1. Inspect the stages on [Spark UI](http://localhost:4040/stages/stage). How many *stages* are necessary to complete the Spark job? What are the roles of `HashAggregate` and `Exchange hashpartitioning`?\n",
    "\n",
    "With Hash aggregation, the database calculates the hash value based on the value following the group by field and maintains the corresponding list in memory based on the aggregation function used earlier. If there are two aggregate functions after select, then two corresponding data are maintained in memory. Similarly, having n aggregation functions will maintain n of the same arrays.\n",
    "\n",
    "Exchange hashpartitioning\n",
    "\n",
    "1. Does the physical plan perform `shuffle` operations? If yes how many?\n",
    "\n",
    "Any operator with reduce requires a shuffle operation.\n",
    "\n",
    "1. What are tasks with respect to stages (in Spark language)? How many tasks are your stages made of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now, compute the following and produce relevant plots:\n",
    "\n",
    "1. Break down the trip distance distribution for each day of week\n",
    "1. Count the number of distinct pickup location\n",
    "1. Compute and display tips and profits as a function of the pickup location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Break down the trip distance distribution for each day of week\n",
    "trip_distance_weekofday = trip_distance_f.groupby('dayofweek','trip_distance_int').count()\n",
    "trip_distance_weekofday.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance = trip_distance_weekofday.orderBy(trip_distance_weekofday['trip_distance_int'],\n",
    "                                                trip_distance_weekofday['dayofweek']).select('*').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(trip_distance, \n",
    "                   x=\"trip_distance_int\", y=\"count\", \n",
    "                   color=\"dayofweek\",nbins=30,\n",
    "                   title=\"the trip distance distribution of day of week\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Count the number of distinct pickup location\n",
    "trip_distance_f.select('pickup_longitude','pickup_latitude').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Compute and display tips and profits as a function of the pickup location\n",
    "\n",
    "token = 'pk.eyJ1Ijoid2VuaHVhbjA0MjEiLCJhIjoiY2wwZW54cmlwMGl0ZTNrazlobmx6eWl4ZSJ9.Fs0GrFewZMDTY7qAvaYmhA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance = df2015_f.where('trip_distance > 0 and trip_distance < 30').select('trip_distance','tpep_pickup_datetime','pickup_longitude','pickup_latitude','dayofweek','tip_amount','total_amount')\n",
    "\n",
    "pickup_df = df2015_add_f.where('pickup_longitude','pickup_latitude','tip_amount','mta_tax','tolls_amount','total_amount','pickup_zone')\n",
    "pickup_df = pickup_df.sample(False,0.0001)\n",
    "pickup_df.withColumn(\"profits\", col = pickup_df(\"total_amount\")-pickup_df(\"mta_tax\")-pickup_df(\"tolls_amount\"))\n",
    "pickup_df.toPandas()\n",
    "pickup_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(pickup_df,\n",
    "                       lon = 'pickup_longitude',\n",
    "                       lat = 'pickup_latitude',\n",
    "                       size = 'tip_amount',\n",
    "                       title = \"The function of tips and pickup location\",\n",
    "                       hover_data = ['pickup_zone'],\n",
    "                       size_max = 30,\n",
    "                       color_continuous_scale = px.colors.carto.Temps)\n",
    "\n",
    "fig.update_layout( mapbox = {'accesstoken':token,'center':{'lon':-73.965691,'lat':40.97},'zoom':5},margin = {'l':1,'r':1,'t':1,'b':1})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(pickup_df,\n",
    "                       lon = 'pickup_longitude',\n",
    "                       lat = 'pickup_latitude',\n",
    "                       size = 'profits',\n",
    "                       title = \"The profit of tips and pickup location\",\n",
    "                       hover_data = ['pickup_zone'],\n",
    "                       size_max = 30,\n",
    "                       color_continuous_scale = px.colors.carto.Temps)\n",
    "\n",
    "fig.update_layout( mapbox = {'accesstoken':token,'center':{'lon':-73.965691,'lat':40.97},'zoom':5},margin = {'l':1,'r':1,'t':1,'b':1})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Investigate one month of trips data in 2015 and 2018\n",
    "\n",
    " Consider one month of trips data from `yellow` taxis for each year\n",
    "\n",
    "1. Filter and cache/persist the result\n",
    "\n",
    "## Assessing seasonalities and looking at time series\n",
    "\n",
    "Compute and plot the following time series indexed by day of the week and hour of day:\n",
    "\n",
    "    1. The number of pickups\n",
    "    2. The average fare\n",
    "    3. The average trip duration\n",
    "    4. Plot the average of ongoing trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:14:54.927863Z",
     "start_time": "2022-03-13T19:14:49.835454Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data pour 4.1\n",
    "time_series2015 = df2015.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'fare_amount').cache()\n",
    "time_series2018 = df2018.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'fare_amount').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:15:16.472013Z",
     "start_time": "2022-03-13T19:15:09.888197Z"
    }
   },
   "outputs": [],
   "source": [
    "time_series2015.count()\n",
    "time_series2015.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:15:19.734390Z",
     "start_time": "2022-03-13T19:15:19.452121Z"
    }
   },
   "outputs": [],
   "source": [
    "time_series2018.count()\n",
    "time_series2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:16:58.415178Z",
     "start_time": "2022-03-13T19:16:58.361382Z"
    }
   },
   "outputs": [],
   "source": [
    "# ajouter des colonnes pour 4.1\n",
    "def set_date(df):\n",
    "    df = df\\\n",
    "        .withColumn('weekofyear', fn.weekofyear('tpep_pickup_datetime'))\\\n",
    "        .withColumn('dayofweek', fn.date_format(col('tpep_pickup_datetime'), \"EEEE\"))\\\n",
    "        .withColumn('date', fn.date_format(col('tpep_pickup_datetime'), \"dd\"))\\\n",
    "        .withColumn('hour', fn.hour('tpep_pickup_datetime'))\\\n",
    "        .withColumn('tripDurationInMinutes', fn.round((col('tpep_dropoff_datetime').cast('long') - col('tpep_pickup_datetime').cast('long'))/60, 2))\n",
    "    return df\n",
    "\n",
    "def addDuration(df):\n",
    "    df = df.withColumn('tripDurationInMinutes', fn.round((col('tpep_dropoff_datetime').cast('long') - col('tpep_pickup_datetime').cast('long'))/60, 2))\n",
    "    return df\n",
    "    \n",
    "# The functions pour 4.1\n",
    "func_num_pickups = fn.count(col('tpep_pickup_datetime'))\n",
    "func_average_fare = fn.round(fn.avg(col('fare_amount')), 2)\n",
    "func_average_duration = fn.round(fn.avg(col('tripDurationInMinutes')), 2)\n",
    "\n",
    "# The function of time series\n",
    "def time_series(df, typeTimeSerie, function):\n",
    "    if (typeTimeSerie == 'day_of_week'):\n",
    "        argGroupBy1 = 'weekofyear'\n",
    "        argGroupBy2 = 'dayofweek'\n",
    "    elif (typeTimeSerie == 'hour_of_day'):\n",
    "        argGroupBy1 = 'date'\n",
    "        argGroupBy2 = 'hour'\n",
    "    else:\n",
    "        print(\"type time serie error\")\n",
    "        return\n",
    "    df_ts = df\\\n",
    "            .groupBy(argGroupBy1, argGroupBy2)\\\n",
    "            .agg(function.alias('num_time_series'))\\\n",
    "            .orderBy(argGroupBy1, argGroupBy2)\n",
    "    return df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:17:35.824390Z",
     "start_time": "2022-03-13T19:17:04.327292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_series2015 = set_date(time_series2015).cache()\n",
    "time_series2015.count()\n",
    "time_series2015.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:17:59.198348Z",
     "start_time": "2022-03-13T19:17:38.701098Z"
    }
   },
   "outputs": [],
   "source": [
    "time_series2018 = set_date(time_series2018).cache()\n",
    "time_series2018.count()\n",
    "time_series2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:18:04.466168Z",
     "start_time": "2022-03-13T19:18:03.428602Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_pickups_dw_2015 = time_series(time_series2015, 'day_of_week', func_num_pickups).cache()\n",
    "num_pickups_hd_2015 = time_series(time_series2015, 'hour_of_day', func_num_pickups).cache()\n",
    "avg_fare_dw_2015 = time_series(time_series2015, 'day_of_week', func_average_fare).cache()\n",
    "avg_fare_hd_2015 = time_series(time_series2015, 'hour_of_day', func_average_fare).cache()\n",
    "avg_trip_dur_dw_2015 = time_series(time_series2015, 'day_of_week', func_average_duration).cache()\n",
    "avg_trip_dur_hd_2015 = time_series(time_series2015, 'hour_of_day', func_average_duration).cache()\n",
    "\n",
    "num_pickups_dw_2018 = time_series(time_series2018, 'day_of_week', func_num_pickups).cache()\n",
    "num_pickups_hd_2018 = time_series(time_series2018, 'hour_of_day', func_num_pickups).cache()\n",
    "avg_fare_dw_2018 = time_series(time_series2018, 'day_of_week', func_average_fare).cache()\n",
    "avg_fare_hd_2018 = time_series(time_series2018, 'hour_of_day', func_average_fare).cache()\n",
    "avg_trip_dur_dw_2018 = time_series(time_series2018, 'day_of_week', func_average_duration).cache()\n",
    "avg_trip_dur_hd_2018 = time_series(time_series2018, 'hour_of_day', func_average_duration).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:18:06.671704Z",
     "start_time": "2022-03-13T19:18:06.651649Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "def new_update_layout(fig,my_title,x,y,my_legend_title=None) :\n",
    "    fig.update_layout(\n",
    "        title={'text' : my_title,\n",
    "               'x':0.5,\n",
    "               'xanchor': 'center'},\n",
    "        legend_title = my_legend_title,\n",
    "        xaxis_title = x,\n",
    "        yaxis_title = y          \n",
    "    )\n",
    "    return\n",
    "\n",
    "def plot_time_series(df, typeTimeSerie, title, xaxis_title, yaxis_title):\n",
    "    if (typeTimeSerie == 'day_of_week'):\n",
    "        xaxis = 'weekofyear'\n",
    "        group = 'dayofweek'\n",
    "    elif (typeTimeSerie == 'hour_of_day'):\n",
    "        xaxis = 'date'\n",
    "        group = 'hour'\n",
    "    else:\n",
    "        print(\"type time serie error\")\n",
    "        return\n",
    "    yaxis = 'num_time_series'\n",
    "    plot_pd = toPandas(df)\n",
    "    fig = px.line(plot_pd, \n",
    "                  x=xaxis, \n",
    "                  y=yaxis, \n",
    "                  color=group, \n",
    "                  line_group=group, \n",
    "                  hover_name=group)\n",
    "    new_update_layout(fig, title, x=xaxis_title, y=yaxis_title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of pickups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:18:43.194191Z",
     "start_time": "2022-03-13T19:18:09.861871Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_time_series(num_pickups_dw_2015, 'day_of_week',\n",
    "                 \"The number of pickups in 2015 by day of week\",\n",
    "                 \"the number of week in 2015\", \"the number of pickups\"\n",
    "                )\n",
    "\n",
    "plot_time_series(num_pickups_hd_2015, 'hour_of_day',\n",
    "                 \"The number of pickups in 2015 by hour of day\",\n",
    "                 \"date\", \"the number of pickups\"\n",
    "                )\n",
    "\n",
    "plot_time_series(num_pickups_dw_2018, 'day_of_week',\n",
    "                 \"The number of pickups in 2018 by day of week\",\n",
    "                 \"the number of week in 2018\", \"the number of pickups\"\n",
    "                )\n",
    "\n",
    "plot_time_series(num_pickups_hd_2018, 'hour_of_day',\n",
    "                 \"The number of pickups in 2018 by hour of day\",\n",
    "                 \"date\", \"the number of pickups\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:19:10.494859Z",
     "start_time": "2022-03-13T19:18:47.150406Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_time_series(avg_fare_dw_2015, 'day_of_week',\n",
    "                 \"The average fare in 2015 by day of week\",\n",
    "                 \"the number of week in 2015\", \"the average fare\"\n",
    "                )\n",
    "\n",
    "plot_time_series(avg_fare_hd_2015, 'hour_of_day',\n",
    "                 \"The average fare in 2015 by hour of day\",\n",
    "                 \"date\", \"the average fare\"\n",
    "                )\n",
    "\n",
    "plot_time_series(avg_fare_dw_2018, 'day_of_week',\n",
    "                 \"The average fare in 2018 by day of week\",\n",
    "                 \"the number of week in 2018\", \"the average fare\"\n",
    "                )\n",
    "\n",
    "plot_time_series(avg_fare_hd_2018, 'hour_of_day',\n",
    "                 \"The average fare in 2018 by hour of day\",\n",
    "                 \"date\", \"the average fare\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:19:32.279825Z",
     "start_time": "2022-03-13T19:19:14.518892Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_time_series(avg_trip_dur_dw_2015, 'day_of_week',\n",
    "                 \"The average trip duration in 2015 by day of week\",\n",
    "                 \"the number of week in 2015\", \"the average trip duration\"\n",
    "                )\n",
    "\n",
    "plot_time_series(avg_trip_dur_hd_2015, 'hour_of_day',\n",
    "                 \"The average trip duration in 2015 by hour of day\",\n",
    "                 \"date\", \"the average trip duration\"\n",
    "                )\n",
    "\n",
    "plot_time_series(avg_trip_dur_dw_2018, 'day_of_week',\n",
    "                 \"The average trip duration in 2018 by day of week\",\n",
    "                 \"the number of week in 2018\", \"the average trip duration\"\n",
    "                )\n",
    "\n",
    "plot_time_series(avg_trip_dur_hd_2018, 'hour_of_day',\n",
    "                 \"The average trip duration in 2018 by hour of day\",\n",
    "                 \"date\", \"the average trip duration\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rides to the airports\n",
    "In order to find the longitude and lattitude of JFK and Newark airport as well as the longitude and magnitudes of Manhattan, you can use a service like geojson.io. Plot the following time series, indexed the day of the week and hour of the day\n",
    "\n",
    "    1  Median duration of taxi trip leaving Midtown (Southern Manhattan) headed for JFK Airport\n",
    "    2  Median taxi duration of trip leaving from JFK Airport to Midtown (Southern Manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T17:40:12.002948Z",
     "start_time": "2022-03-04T17:40:08.881834Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:19:36.346017Z",
     "start_time": "2022-03-13T19:19:36.320530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coordonnées des zones spécifiques\n",
    "# Southern Manhattan\n",
    "South_Manhattan = {\"type\": \"Polygon\",\n",
    "                   \"coordinates\": [[\n",
    "                                [-73.99755477905273,40.77313187935118],\n",
    "                                [-74.01420593261717,40.751418432997454],\n",
    "                                [-74.02210235595702,40.70549780669077],\n",
    "                                [-74.01540756225586,40.69847032728747],\n",
    "                                [-73.97592544555664,40.70992213555912],\n",
    "                                [-73.96905899047852,40.73177921058233],\n",
    "                                [-73.9712905883789,40.74127439314326],\n",
    "                                [-73.95824432373047,40.758700379161006],\n",
    "                                [-73.99703979492188,40.774041868909734],\n",
    "                                [-73.99755477905273,40.77313187935118]]]\n",
    "                  }\n",
    "South_Manhattan_id = [12,88,87,261,13,209,231,45,232,148,144,211,125,114,158,249,113,79,4,224,107,234,90,68,246,186,164,170,137,233,229,162,161,230,48,50,163,100]\n",
    "\n",
    "# JFK Airport\n",
    "JFK = {\"type\": \"Polygon\",\n",
    "       \"coordinates\": [[\n",
    "                       [-73.78778457641602,40.666251560504264],\n",
    "                       [-73.8226318359375,40.66475414828327],\n",
    "                       [-73.82649421691895,40.6546620153016],\n",
    "                       [-73.8226318359375,40.64469860601899],\n",
    "                       [-73.78469467163085,40.61864344909241],\n",
    "                       [-73.77053260803221,40.61949040153005],\n",
    "                       [-73.74701499938965,40.634994248282894],\n",
    "                       [-73.74881744384766,40.64632671574881],\n",
    "                       [-73.78778457641602,40.666251560504264]]]\n",
    "      }\n",
    "JFK_id = [132]\n",
    "\n",
    "# LaGuardia Airport\n",
    "LaGuardia = {\"type\": \"Polygon\",\n",
    "             \"coordinates\": [[\n",
    "                                [-73.88777732849121,40.76656658538413],\n",
    "                                [-73.8758897781372,40.77144186567577],\n",
    "                                [-73.86966705322266,40.7709543537425],\n",
    "                                [-73.86297225952148,40.76630656038832],\n",
    "                                [-73.85541915893555,40.76737915693862],\n",
    "                                [-73.85404586791992,40.77313187935118],\n",
    "                                [-73.87056827545166,40.787040358887566],\n",
    "                                [-73.89009475708008,40.77842914365316],\n",
    "                                [-73.88777732849121,40.76656658538413]]]\n",
    "                 }\n",
    "LaGuardia_id = [138]\n",
    "\n",
    "# Newark Airport\n",
    "Newark = {\"type\": \"Polygon\",\n",
    "          \"coordinates\": [[\n",
    "                          [-74.15797233581543,40.708295578231315],\n",
    "                          [-74.18097496032715,40.70823051511181],\n",
    "                          [-74.19479370117188,40.684738575525],\n",
    "                          [-74.19118881225586,40.67523532779746],\n",
    "                          [-74.17745590209961,40.66911608150882],\n",
    "                          [-74.15119171142578,40.70634365699408],\n",
    "                          [-74.15797233581543,40.708295578231315]]]\n",
    "         }\n",
    "\n",
    "Newark_id = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:19:55.747105Z",
     "start_time": "2022-03-13T19:19:43.289064Z"
    }
   },
   "outputs": [],
   "source": [
    "# data for 4.2\n",
    "airports2015 = df2015.select('tpep_pickup_datetime', 'tpep_dropoff_datetime','pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude').cache()\n",
    "airports2018 = df2018.select('tpep_pickup_datetime', 'tpep_dropoff_datetime','PULocationID', 'DOLocationID').cache()\n",
    "airports2015.count()\n",
    "airports2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:20:13.087741Z",
     "start_time": "2022-03-13T19:20:13.057958Z"
    }
   },
   "outputs": [],
   "source": [
    "# vérifier si un point est dans un polygone\n",
    "import shapely.geometry\n",
    "def in_area_Midtown(longitude, latitude):\n",
    "    point = shapely.geometry.Point(longitude, latitude)\n",
    "    poly_shape = shapely.geometry.asShape(South_Manhattan)\n",
    "    return poly_shape.intersects(point)\n",
    "\n",
    "def in_area_JFK(longitude, latitude):\n",
    "    point = shapely.geometry.Point(longitude, latitude)\n",
    "    poly_shape = shapely.geometry.asShape(JFK)\n",
    "    return poly_shape.intersects(point)\n",
    "\n",
    "def in_area_airport(longitude, latitude):\n",
    "    point = shapely.geometry.Point(longitude, latitude)\n",
    "    poly_shape_JFK = shapely.geometry.asShape(JFK)\n",
    "    poly_shape_LaGuardia = shapely.geometry.asShape(LaGuardia)\n",
    "    poly_shape_Newark = shapely.geometry.asShape(Newark)\n",
    "    return (poly_shape_JFK.intersects(point) | poly_shape_LaGuardia.intersects(point) | poly_shape_Newark.intersects(point))\n",
    "\n",
    "udf_in_area_Midtown = udf(lambda x,y : in_area_Midtown(x,y), BooleanType())\n",
    "udf_in_area_JFK = udf(lambda x,y : in_area_JFK(x,y), BooleanType())\n",
    "udf_in_area_airport = udf(lambda x,y : in_area_airport(x,y), BooleanType())\n",
    "\n",
    "\n",
    "\n",
    "# ajouter des colonnes si la location est dans un domaine donné\n",
    "def add_area_2015(df):\n",
    "    df = df\\\n",
    "        .withColumn('pickup_Midtown', udf_in_area_Midtown(col('pickup_longitude'), col('pickup_latitude')))\\\n",
    "        .withColumn('dropoff_JFK', udf_in_area_JFK(col('dropoff_longitude'), col('dropoff_latitude')))\\\n",
    "        .withColumn('pickup_JFK', udf_in_area_JFK(col('pickup_longitude'), col('pickup_latitude')))\\\n",
    "        .withColumn('dropoff_Midtown', udf_in_area_Midtown(col('dropoff_longitude'), col('dropoff_latitude')))\n",
    "    return df\n",
    "\n",
    "def midtown_to_JFK2015(df):\n",
    "    df = df\\\n",
    "        .withColumn('pickup_Midtown', udf_in_area_Midtown(col('pickup_longitude'), col('pickup_latitude')))\\\n",
    "        .withColumn('dropoff_JFK', udf_in_area_JFK(col('dropoff_longitude'), col('dropoff_latitude')))\n",
    "    return df\n",
    "\n",
    "def JFK_to_midtown2015(df):\n",
    "    df = df\\\n",
    "        .withColumn('pickup_JFK', udf_in_area_JFK(col('pickup_longitude'), col('pickup_latitude')))\\\n",
    "        .withColumn('dropoff_Midtown', udf_in_area_Midtown(col('dropoff_longitude'), col('dropoff_latitude')))\n",
    "    return df\n",
    "\n",
    "def in_id_Midtown(ID):\n",
    "    return ID in South_Manhattan_id\n",
    "\n",
    "def in_id_JFK(ID):\n",
    "    return ID in JFK_id\n",
    "    \n",
    "udf_in_id_Midtown = udf(lambda x : in_id_Midtown(x), BooleanType())\n",
    "udf_in_id_JFK = udf(lambda x : in_id_JFK(x), BooleanType())\n",
    "\n",
    "def add_area_2018(df):\n",
    "    df = df\\\n",
    "        .withColumn('pickup_Midtown',udf_in_id_Midtown(col('PULocationID')))\\\n",
    "        .withColumn('dropoff_JFK',udf_in_id_JFK(col('DOLocationID')))\\\n",
    "        .withColumn('pickup_JFK',udf_in_id_JFK(col('PULocationID')))\\\n",
    "        .withColumn('dropoff_Midtown',udf_in_id_Midtown(col('DOLocationID')))\n",
    "    return df\n",
    "\n",
    "def midtown_to_JFK2018(df):\n",
    "    df = df\\\n",
    "        .withColumn('pickup_Midtown', udf_in_id_Midtown(col('PULocationID')))\\\n",
    "        .withColumn('dropoff_JFK', udf_in_id_JFK(col('DOLocationID')))\n",
    "    return df\n",
    "\n",
    "def JFK_to_midtown2018(df):\n",
    "    df = df\\\n",
    "        .withColumn('pickup_JFK', udf_in_id_JFK(col('PULocationID')))\\\n",
    "        .withColumn('dropoff_Midtown', udf_in_id_Midtown(col('DOLocationID')))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:21:55.163029Z",
     "start_time": "2022-03-13T19:21:01.772551Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "airports_dur2015 = set_date(airports2015).cache()\n",
    "airports_dur2018 = set_date(airports2018).cache()\n",
    "airports_dur2015.count()\n",
    "airports_dur2018.count()\n",
    "airports_dur2015.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:22:00.018381Z",
     "start_time": "2022-03-13T19:21:59.718596Z"
    }
   },
   "outputs": [],
   "source": [
    "airports_dur2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:23:25.247845Z",
     "start_time": "2022-03-13T19:22:29.756314Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "area_airports_dur2018 = add_area_2018(airports_dur2018)\n",
    "area_airports_dur2018.cache()\n",
    "area_airports_dur2018.count()\n",
    "area_airports_dur2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T10:36:32.101129Z",
     "start_time": "2022-03-12T10:36:31.988688Z"
    }
   },
   "outputs": [],
   "source": [
    "# trop lent !!!!!\n",
    "area_airports_dur2015 = airports_dur2015.repartition(30)\n",
    "area_airports_dur2015 = add_area_2015(area_airports_dur2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:36:58.217142Z",
     "start_time": "2022-03-13T19:36:58.210839Z"
    }
   },
   "outputs": [],
   "source": [
    "def MedianDuration(df, typeTimeSerie, depart, arrival, orderby):\n",
    "    if (typeTimeSerie == 'day_of_week'):\n",
    "        arg1 = 'weekofyear'\n",
    "        arg2 = 'dayofweek'\n",
    "    elif (typeTimeSerie == 'hour_of_day'):\n",
    "        arg1 = 'date'\n",
    "        arg2 = 'hour'\n",
    "    else:\n",
    "        print(\"type time serie error\")\n",
    "        return\n",
    "    window = Window.partitionBy(arg1, arg2)\n",
    "    med = fn.expr('percentile_approx(tripDurationInMinutes, 0.5)').over(window)\n",
    "    df = df.where(col(depart)).where(col(arrival))\n",
    "    df = df.withColumn('num_time_series', med)\n",
    "    df = df.select(arg1, arg2, 'num_time_series')\n",
    "    df = df.dropDuplicates([arg1, arg2])\n",
    "    df = df.orderBy(orderby)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:37:02.068061Z",
     "start_time": "2022-03-13T19:36:59.866858Z"
    }
   },
   "outputs": [],
   "source": [
    "med_JFK_to_Mid_dw_2018 = MedianDuration(area_airports_dur2018, 'day_of_week', 'pickup_JFK', 'dropoff_Midtown', 'weekofyear')\n",
    "med_JFK_to_Mid_dw_2018.cache()\n",
    "med_JFK_to_Mid_dw_2018.count()\n",
    "med_JFK_to_Mid_dw_2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:37:06.508865Z",
     "start_time": "2022-03-13T19:37:03.853425Z"
    }
   },
   "outputs": [],
   "source": [
    "med_JFK_to_Mid_hd_2018 = MedianDuration(area_airports_dur2018, 'hour_of_day', 'pickup_JFK', 'dropoff_Midtown', 'date')\n",
    "med_JFK_to_Mid_hd_2018.cache()\n",
    "med_JFK_to_Mid_hd_2018.count()\n",
    "med_JFK_to_Mid_hd_2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:37:13.289546Z",
     "start_time": "2022-03-13T19:37:11.546644Z"
    }
   },
   "outputs": [],
   "source": [
    "med_mid_to_JFK_dw_2018 = MedianDuration(area_airports_dur2018, 'day_of_week', 'pickup_Midtown', 'dropoff_JFK', 'weekofyear')\n",
    "med_mid_to_JFK_dw_2018.cache()\n",
    "med_mid_to_JFK_dw_2018.count()\n",
    "med_mid_to_JFK_dw_2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:37:21.162058Z",
     "start_time": "2022-03-13T19:37:19.548304Z"
    }
   },
   "outputs": [],
   "source": [
    "med_mid_to_JFK_hd_2018 = MedianDuration(area_airports_dur2018, 'hour_of_day', 'pickup_Midtown', 'dropoff_JFK', 'date')\n",
    "med_mid_to_JFK_hd_2018.cache()\n",
    "med_mid_to_JFK_hd_2018.count()\n",
    "med_mid_to_JFK_hd_2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T10:41:59.709958Z",
     "start_time": "2022-03-12T10:36:43.154098Z"
    }
   },
   "outputs": [],
   "source": [
    "# trop lent !!!!\n",
    "med_JFK_to_mid_dw_2015 = MedianDuration(area_airports_dur2015, 'day_of_week', 'pickup_JFK', 'dropoff_Midtown')\n",
    "med_JFK_to_mid_dw_2015.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median duration of taxi trip leaving Midtown (Southern Manhattan) headed for JFK Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:37:26.640015Z",
     "start_time": "2022-03-13T19:37:24.948290Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_time_series(med_mid_to_JFK_dw_2018, \n",
    "                 'day_of_week',\n",
    "                 \"Median duration of taxi trip leaving Midtown headed for JFK Airport in 2018 by day of week\",\n",
    "                 \"week of year\", \n",
    "                 \"the median trip duration\"\n",
    "                )\n",
    "\n",
    "plot_time_series(med_mid_to_JFK_hd_2018, \n",
    "                 'hour_of_day',\n",
    "                 \"Median duration of taxi trip leaving Midtown headed for JFK Airport in 2018 by hour of day\",\n",
    "                 \"date\", \n",
    "                 \"the median trip duration\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median taxi duration of trip leaving from JFK Airport to Midtown (Southern Manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:39:51.707253Z",
     "start_time": "2022-03-13T19:39:50.079443Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_time_series(med_JFK_to_Mid_dw_2018, \n",
    "                 'day_of_week',\n",
    "                 \"Median duration of taxi trip leaving from JFK Airport to Midtown in 2018 by day of week\",\n",
    "                 \"week of year\", \n",
    "                 \"the median trip duration\"\n",
    "                )\n",
    "\n",
    "plot_time_series(med_JFK_to_Mid_hd_2018, \n",
    "                 'hour_of_day',\n",
    "                 \"Median duration of taxi trip leaving from JFK Airport to Midtown in 2018 by hour of day\",\n",
    "                 \"date\", \n",
    "                 \"the median trip duration\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic information\n",
    "\n",
    "For this, you will need to find tools to display maps and to build choropeth maps.\n",
    "We let you look and find relevant tools to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a heatmap where color is a function of\n",
    "    1. number of `pickups`\n",
    "    2. number of `dropoffs`\n",
    "    3. number of `pickups` with dropoff at some airport (JFK, LaGuardia, Newark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T10:41:59.648457Z",
     "start_time": "2022-03-06T10:41:25.559068Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install datashader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:41:09.225010Z",
     "start_time": "2022-03-13T19:40:02.606140Z"
    }
   },
   "outputs": [],
   "source": [
    "# data for 4.3.1\n",
    "pickup_dropoff2015 = df2015.select('pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude')\n",
    "pickup_dropoff2015 = pickup_dropoff2015.cache()\n",
    "pickup_dropoff2015.count()\n",
    "pd_pickup_dropoff2015 = toPandas(pickup_dropoff2015, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:41:27.438663Z",
     "start_time": "2022-03-13T19:41:22.595392Z"
    }
   },
   "outputs": [],
   "source": [
    "pickup_dropoff2018 = df2018.select('PULocationID','DOLocationID')\n",
    "pickup_dropoff2018.cache()\n",
    "pickup_dropoff2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:41:56.077307Z",
     "start_time": "2022-03-13T19:41:42.869719Z"
    }
   },
   "outputs": [],
   "source": [
    "dropoff2018 = df2018.select('DOLocationID').groupBy('DOLocationID').agg(fn.count('DOLocationID').alias('numdropoff'))\n",
    "dropoff2018.cache()\n",
    "dropoff2018.count()\n",
    "pd_dropoff2018 = toPandas(dropoff2018)\n",
    "pickup2018 = df2018.select('PULocationID').groupBy('PULocationID').agg(fn.count('PULocationID').alias('numpickup'))\n",
    "pickup2018.cache()\n",
    "pickup2018.count()\n",
    "pd_pickup2018 = toPandas(pickup2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:42:07.218697Z",
     "start_time": "2022-03-13T19:42:03.490044Z"
    }
   },
   "outputs": [],
   "source": [
    "# data of the NYC taxi zone\n",
    "urllib.request.urlretrieve(\"https://data.cityofnewyork.us/api/geospatial/d3c5-ddgc?method=export&format=Shapefile\", \"NYC Taxi Zones.zip\")\n",
    "with zipfile.ZipFile(\"NYC Taxi Zones.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./shape\")\n",
    "shp_path = os.path.join('shape/geo_export_6b025b64-5fe2-42d7-8e38-5a8d46c76b0d.shp')\n",
    "gdf_zones = geopandas.GeoDataFrame.from_file(shp_path)\n",
    "gdf_zones.to_crs(epsg=4326,inplace=True)\n",
    "gdf_zones['point'] = gdf_zones.representative_point()\n",
    "gdf_zones['location_i'] = gdf_zones[\"location_i\"].astype(\"int\")\n",
    "gdf_zones[\"lon\"] = gdf_zones['point'].x\n",
    "gdf_zones[\"lat\"] = gdf_zones['point'].y\n",
    "loc_zones = gdf_zones[['location_i','lon','lat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:42:12.807010Z",
     "start_time": "2022-03-13T19:42:12.758793Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_pickup2018 = pd.merge(pd_pickup2018, loc_zones, left_on='PULocationID', right_on='location_i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:42:15.231905Z",
     "start_time": "2022-03-13T19:42:15.172389Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_pickup2018.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:42:17.689791Z",
     "start_time": "2022-03-13T19:42:17.684659Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_dropoff2018 = pd.merge(pd_dropoff2018, loc_zones, left_on='DOLocationID', right_on='location_i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:42:21.238929Z",
     "start_time": "2022-03-13T19:42:21.221421Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_dropoff2018.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:42:40.003757Z",
     "start_time": "2022-03-13T19:42:30.475340Z"
    }
   },
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "from colorcet import fire\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "def plot_heatmap(df, up_off):\n",
    "    if (up_off == 'pickup'):\n",
    "        Lat = 'pickup_latitude'\n",
    "        Lon = 'pickup_longitude'\n",
    "        dff = df\\\n",
    "            .query('pickup_latitude < 40.90')\\\n",
    "            .query('pickup_latitude > 40.58')\\\n",
    "            .query('pickup_longitude > -74.10')\\\n",
    "            .query('pickup_longitude < -73.70')\n",
    "    if (up_off == 'dropoff'):\n",
    "        Lat = 'dropoff_latitude'\n",
    "        Lon = 'dropoff_longitude'\n",
    "        dff = df\\\n",
    "            .query('dropoff_latitude < 40.90')\\\n",
    "            .query('dropoff_latitude > 40.58')\\\n",
    "            .query('dropoff_longitude > -74.10')\\\n",
    "            .query('dropoff_longitude < -73.70')\n",
    "\n",
    "    cvs = ds.Canvas(plot_width=1000, plot_height=1000)\n",
    "    agg = cvs.points(dff, x=Lon, y=Lat)\n",
    "\n",
    "    coords_lat, coords_lon = agg.coords[Lat].values, agg.coords[Lon].values\n",
    "\n",
    "    coordinates = [[coords_lon[0], coords_lat[0]],\n",
    "                   [coords_lon[-1], coords_lat[0]],\n",
    "                   [coords_lon[-1], coords_lat[-1]],\n",
    "                   [coords_lon[0], coords_lat[-1]]]\n",
    "\n",
    "    img = tf.shade(agg, cmap=fire)[::-1].to_pil()\n",
    "\n",
    "    fig = px.scatter_mapbox(dff[:1], lat=Lat, lon=Lon, zoom=10)\n",
    "    fig.update_layout(mapbox_style=\"carto-darkmatter\",\n",
    "                     mapbox_layers = [\n",
    "                    {\n",
    "                        \"sourcetype\": \"image\",\n",
    "                        \"source\": img,\n",
    "                        \"coordinates\": coordinates\n",
    "                    }]\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:42:43.360043Z",
     "start_time": "2022-03-13T19:42:43.347675Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_heatmap18(df, num, title):    \n",
    "    fig = px.density_mapbox(\n",
    "        df, lat='lat', lon='lon', z=num, radius=10,\n",
    "        center=dict(lat=40.74, lon=-73.96), zoom=10,\n",
    "        color_continuous_scale=\"Viridis\",\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title = title,   \n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of pickups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:43:03.301164Z",
     "start_time": "2022-03-13T19:42:49.751791Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_heatmap(pd_pickup_dropoff2015, 'pickup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:43:11.659727Z",
     "start_time": "2022-03-13T19:43:11.085095Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_heatmap18(pd_pickup2018, 'numpickup', 'Number of pickups in July 2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of dropoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:43:24.738724Z",
     "start_time": "2022-03-13T19:43:21.039359Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_heatmap(pd_pickup_dropoff2015, 'dropoff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:43:33.022443Z",
     "start_time": "2022-03-13T19:43:32.932740Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_heatmap18(pd_dropoff2018, 'numdropoff', 'Number of dropoff in July 2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of pickups with dropoff at some airport (JFK, LaGuardia, Newark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:43:48.559750Z",
     "start_time": "2022-03-13T19:43:41.189745Z"
    }
   },
   "outputs": [],
   "source": [
    "to_airport2018 = pickup_dropoff2018.where(\"DOLocationID == 1 or DOLocationID == 132 or DOLocationID == 138\")\n",
    "numPickup_to_airport2018 = to_airport2018.groupBy('PULocationID').agg(fn.count('PULocationID').alias('numPickups'))\n",
    "numPickup_to_airport2018.cache()\n",
    "numPickup_to_airport2018.count()\n",
    "pd_Pickup_to_airport2018 = toPandas(numPickup_to_airport2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:43:51.439117Z",
     "start_time": "2022-03-13T19:43:51.418998Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_Pickup_to_airport2018 = pd.merge(pd_Pickup_to_airport2018, loc_zones, \n",
    "                                    left_on='PULocationID', right_on='location_i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:43:53.530267Z",
     "start_time": "2022-03-13T19:43:53.507727Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_Pickup_to_airport2018.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:44:02.724700Z",
     "start_time": "2022-03-13T19:44:02.632383Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_heatmap18(pd_Pickup_to_airport2018, 'numPickups', \n",
    "               'Number of pickups with dropoff at some airport in July 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T11:31:12.633206Z",
     "start_time": "2022-03-13T10:30:12.316439Z"
    }
   },
   "outputs": [],
   "source": [
    "# trop lent !!!!\n",
    "pickup_dropoffAirport2015 = pickup_dropoff2015.repartition(100)\n",
    "pickup_dropoffAirport2015 = pickup_dropoffAirport2015.withColumn('dropoff_in_airport', udf_in_area_airport(col('dropoff_longitude'), col('dropoff_latitude')))\n",
    "pickup_dropoffAirport2015 = pickup_dropoffAirport2015.cache()\n",
    "pickup_dropoffAirport2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T21:20:00.338231Z",
     "start_time": "2022-03-12T21:20:00.113342Z"
    }
   },
   "outputs": [],
   "source": [
    "# trop lent !!!!\n",
    "numpick_dropAirport2015 = pickup_dropoffAirport2015.select('pickup_longitude', 'pickup_latitude').where(col('dropoff_in_airport'))\n",
    "numpick_dropAirport2015.persist()\n",
    "numpick_dropAirport2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T21:31:06.045881Z",
     "start_time": "2022-03-12T21:27:08.871094Z"
    }
   },
   "outputs": [],
   "source": [
    "# trop lent !!!!\n",
    "pd_numPick_Airport2015 = toPandas(numpick_dropAirport2015, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trop lent !!!!\n",
    "plot_heatmap(pd_pickup_dropoffAirport2015, 'dropoff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a choropeth map where color is a function of\n",
    "    1. number of pickups in the area\n",
    "    2. ratio of number of payments by card/number of cash payments for pickups in the area\n",
    "    3. ratio of total fare/trip duration for dropoff in the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:44:17.478526Z",
     "start_time": "2022-03-13T19:44:15.027643Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the data of NYC Taxi Zones\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/PetitPoissonL/NYC_Taxi_Zones/main/NYC_Taxi_Zones.json') as response:\n",
    "    taxi_zone = json.load(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:44:19.103164Z",
     "start_time": "2022-03-13T19:44:19.095249Z"
    }
   },
   "outputs": [],
   "source": [
    "def choropleth_map(df, col, label, loc, t):\n",
    "    fig = px.choropleth_mapbox(\n",
    "        df, geojson=taxi_zone, locations=loc, color=col,\n",
    "        featureidkey=\"properties.location_id\",\n",
    "        color_continuous_scale=\"Viridis\",\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        zoom=9.5, center = {\"lat\": 40.74, \"lon\": -73.96},\n",
    "        opacity=0.5,\n",
    "        title = t,\n",
    "        labels={col:label}\n",
    "    )\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of pickups in the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:44:28.079500Z",
     "start_time": "2022-03-13T19:44:25.640756Z"
    }
   },
   "outputs": [],
   "source": [
    "# data for 4.3.2.1\n",
    "pickup2018 = df2018.groupBy('PULocationID').agg(fn.count('tpep_pickup_datetime').alias('numPickup'))\n",
    "pd_pickup2018 = toPandas(pickup2018, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:44:35.598667Z",
     "start_time": "2022-03-13T19:44:31.512352Z"
    }
   },
   "outputs": [],
   "source": [
    "choropleth_map(pd_pickup2018, 'numPickup', \n",
    "               'number of pickups', 'PULocationID', \n",
    "               \"Number of pickups in 2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of pickups in the area 2015\n",
    "pickup_zone = df2015_add.groupby('pickup_zone').count()\n",
    "pickup_zone = pickup_zone.toPandas()\n",
    "pickup_zone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chor_pickup = go.Figure(\n",
    "    go.Choroplethmapbox(\n",
    "        geojson = taxi_zone,\n",
    "        featureidkey=\"properties.zone\",\n",
    "        locations = pickup_zone['pickup_zone'],\n",
    "        z = pickup_zone['count'],\n",
    "        zauto=True,\n",
    "        colorscale='ylorrd',\n",
    "        marker={'opacity':0.8,'line_width':0.5},\n",
    "        hovertext = pickup_zone['pickup_zone'],\n",
    "        hoverinfo='text + z',\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "chor_pickup.update_layout( mapbox = {'accesstoken':token,'center':{'lon':-73.965691,'lat':40.97},'zoom':5},margin = {'l':1,'r':1,'t':1,'b':1})\n",
    "chor_pickup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ratio of number of payments by card/number of cash payments for pickups in the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:44:51.446239Z",
     "start_time": "2022-03-13T19:44:46.014684Z"
    }
   },
   "outputs": [],
   "source": [
    "# data for 4.3.2.2\n",
    "ratioPayment2018 = df2018.select('tpep_pickup_datetime', 'PULocationID', 'payment_type').cache()\n",
    "ratioPayment2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:01.207339Z",
     "start_time": "2022-03-13T19:44:54.550488Z"
    }
   },
   "outputs": [],
   "source": [
    "def payment(df, typepatment, nameCol):\n",
    "    df = df\\\n",
    "         .where(col('payment_type')==typepatment)\\\n",
    "         .groupBy(col('PULocationID'))\\\n",
    "         .agg(fn.count(col('tpep_pickup_datetime')).alias(nameCol))\n",
    "    return df\n",
    "    \n",
    "ratioPayment_byCard = payment(ratioPayment2018, 1, 'num_by_card')\n",
    "ratioPayment_byCash = payment(ratioPayment2018, 2, 'num_by_cash')\n",
    "    \n",
    "df_ratioPayment = ratioPayment_byCash.join(ratioPayment_byCard, on=\"PULocationID\")\n",
    "\n",
    "df_ratioPayment = df_ratioPayment\\\n",
    "                  .withColumn('ratio',fn.round(fn.col('num_by_card')/fn.col('num_by_cash'),2))\n",
    "df_ratioPayment.cache()\n",
    "df_ratioPayment.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:05.522129Z",
     "start_time": "2022-03-13T19:45:03.108559Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_ratioPayment = toPandas(df_ratioPayment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:07.978366Z",
     "start_time": "2022-03-13T19:45:07.958461Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_ratioPayment.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:13.979034Z",
     "start_time": "2022-03-13T19:45:10.397728Z"
    }
   },
   "outputs": [],
   "source": [
    "choropleth_map(pd_ratioPayment, 'ratio', \n",
    "               'ration payments', 'PULocationID', \n",
    "               'Ratio of number of payments by card/number of cash payments for pickups in 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio 2015\n",
    "ratiopay_zone = df2015_add.groupby('pickup_zone','payment_type').count().toPandas()\n",
    "ratiopay_zone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_payment = ratiopay_zone[ratiopay_zone['payment_type'] == 1]\n",
    "\n",
    "cash_payment = ratiopay_zone[ratiopay_zone['payment_type'] == 2]\n",
    "\n",
    "payment_df = pd.merge(card_payment,cash_payment,how='inner',on='pickup_zone')\n",
    "payment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df['ratio'] = payment_df['count_x']/payment_df['count_y']\n",
    "payment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_pickup = go.Figure(\n",
    "    go.Choroplethmapbox(\n",
    "        geojson = taxi_zone,\n",
    "        featureidkey=\"properties.zone\",\n",
    "        locations = payment_df['pickup_zone'],\n",
    "        z = payment_df['ratio'],\n",
    "        zauto=True,\n",
    "        colorscale='ylorrd',\n",
    "        marker={'opacity':0.8,'line_width':0.5},\n",
    "        hovertext = payment_df['pickup_zone'],\n",
    "        hoverinfo='text + z',\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "payment_pickup.update_layout( mapbox = {'accesstoken':token,'center':{'lon':-73.965691,'lat':40.97},'zoom':5},margin = {'l':1,'r':1,'t':1,'b':1})\n",
    "payment_pickup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T17:22:46.659102Z",
     "start_time": "2022-03-06T17:22:46.638491Z"
    }
   },
   "source": [
    "#### ratio of total fare/trip duration for dropoff in the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:36.031454Z",
     "start_time": "2022-03-13T19:45:25.937727Z"
    }
   },
   "outputs": [],
   "source": [
    "ratio_fare_dur2018 = df2018.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'DOLocationID', 'total_amount')\n",
    "ratio_fare_dur2018 = addDuration(ratio_fare_dur2018).cache()\n",
    "ratio_fare_dur2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:41.624548Z",
     "start_time": "2022-03-13T19:45:38.667723Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ratio_fare_dur2018 = ratio_fare_dur2018\\\n",
    "                        .groupBy('DOLocationID')\\\n",
    "                        .agg(fn.round(fn.sum('total_amount'),2).alias('total_fare'),\n",
    "                             fn.round(fn.sum('tripDurationInMinutes'),2).alias('trip_duration'))\n",
    "df_ratio_fare_dur2018 = df_ratio_fare_dur2018\\\n",
    "                        .withColumn('ratio',fn.round(fn.col('total_fare')/fn.col('trip_duration'),2))\n",
    "pd_ratio_fare_dur2018 = toPandas(df_ratio_fare_dur2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:45.097434Z",
     "start_time": "2022-03-13T19:45:45.086451Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_ratio_fare_dur2018.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:45:51.325827Z",
     "start_time": "2022-03-13T19:45:47.909578Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "choropleth_map(pd_ratio_fare_dur2018, 'ratio', \n",
    "               'total fare/trip duration in 2018', 'DOLocationID', \n",
    "               'Ratio of total fare/trip duration for dropoff in 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015\n",
    "from pyspark.sql.functions import dayofweek,hour,date_format,unix_timestamp,sum\n",
    "\n",
    "ratiofd_zone = df2015_add.withColumn('trip_duration',(unix_timestamp(df2015_add['tpep_dropoff_datetime']) - unix_timestamp(df2015_add['tpep_pickup_datetime'])))\n",
    "\n",
    "ratiofd_zone = ratiofd_zone.groupby('dropoff_zone').agg(sum(ratiofd_zone['trip_duration']),sum(ratiofd_zone['fare_amount'])).toPandas()\n",
    "ratiofd_zone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratiofd_zone['ratio'] = ratiofd_zone['sum(fare_amount)']/ratiofd_zone['sum(trip_duration)']\n",
    "ratiofd_zone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratiofd_dropoff = go.Figure(\n",
    "    go.Choroplethmapbox(\n",
    "        geojson = taxi_zone,\n",
    "        featureidkey=\"properties.zone\",\n",
    "        locations = ratiofd_zone['dropoff_zone'],\n",
    "        z = ratiofd_zone['ratio'],\n",
    "        zauto=True,\n",
    "        colorscale='ylorrd',\n",
    "        marker={'opacity':0.8,'line_width':0.5},\n",
    "        hovertext = ratiofd_zone['dropoff_zone'],\n",
    "        hoverinfo='text + z',\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "ratiofd_dropoff.update_layout( mapbox = {'accesstoken':token,'center':{'lon':-73.965691,'lat':40.97},'zoom':5},margin = {'l':1,'r':1,'t':1,'b':1})\n",
    "ratiofd_dropoff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an interactive chorophet with a slider \n",
    "    allowing the user to select an `hour of day` and where the color is a function of\n",
    "    1. average number of dropoffs in the area during that hour the day\n",
    "    2. average ratio of tip over total fare amount for pickups in the area at given hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:46:22.805301Z",
     "start_time": "2022-03-13T19:45:56.907066Z"
    }
   },
   "outputs": [],
   "source": [
    "interactive2018 = df2018.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'tip_amount', 'total_amount')\n",
    "interactive2018 = set_date(interactive2018)\n",
    "interactive2018 = interactive2018.persist()\n",
    "interactive2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:46:31.507452Z",
     "start_time": "2022-03-13T19:46:31.043640Z"
    }
   },
   "outputs": [],
   "source": [
    "interactive2018.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour,date_format\n",
    "\n",
    "df2015_add = df2015_add.withColumn('hour_pickup',hour(df2015_add['tpep_pickup_datetime']))\n",
    "df2015_add = df2015_add.withColumn('hour_dropoff',hour(df2015_add['tpep_dropoff_datetime']))\n",
    "df2015_add = df2015_add.withColumn('date_pickup',date_format(df2015_add['tpep_pickup_datetime'],'yyyy-MM-dd'))\n",
    "df2015_add = df2015_add.withColumn('date_dropoff',date_format(df2015_add['tpep_dropoff_datetime'],'yyyy-MM-dd'))\n",
    "df2015_add.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### average number of dropoffs in the area during that hour the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:46:48.014668Z",
     "start_time": "2022-03-13T19:46:47.910483Z"
    }
   },
   "outputs": [],
   "source": [
    "dropoff_interactive2018 = interactive2018\\\n",
    "                          .groupBy('date', 'hour', 'DOLocationID')\\\n",
    "                          .agg(fn.count(col('tpep_dropoff_datetime')).alias('numDropoff'))\\\n",
    "                          .orderBy('date', 'hour')\n",
    "avgdropoff2018 = dropoff_interactive2018\\\n",
    "                          .groupBy('hour', 'DOLocationID')\\\n",
    "                          .agg(fn.round(fn.avg(col('numDropoff')),2).alias('avgDropoff'))\\\n",
    "                          .orderBy('hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:47:15.607948Z",
     "start_time": "2022-03-13T19:46:49.433253Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_avgdropoff2018 = toPandas(avgdropoff2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:47:19.957277Z",
     "start_time": "2022-03-13T19:47:19.931577Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_avgdropoff2018.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:49:26.212650Z",
     "start_time": "2022-03-13T19:47:38.941834Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(\n",
    "    pd_avgdropoff2018, \n",
    "    geojson=taxi_zone, locations='DOLocationID', color='avgDropoff',\n",
    "    featureidkey=\"properties.location_id\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    zoom=9.5, center = {\"lat\": 40.74, \"lon\": -73.96},\n",
    "    animation_frame=\"hour\",\n",
    "    opacity=0.5,\n",
    "    labels={'avgDropoff':'average number of dropoffs'},\n",
    "    title = 'average number of dropoffs in 2018'\n",
    ")\n",
    "\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015\n",
    "avg_dropoffs = df2015_add.groupby('dropoff_zone','hour_dropoff','date_dropoff').count()\n",
    "avg_dropoffs = avg_dropoffs.groupby('dropoff_zone','hour_dropoff').agg({'count':'mean'}).toPandas()\n",
    "avg_dropoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "datas = []\n",
    "\n",
    "for hour in range(24):\n",
    "    t = avg_dropoffs[avg_dropoffs['hour_dropoff'] == hour]\n",
    "    data = go.Choroplethmapbox(\n",
    "        geojson = taxi_zone,\n",
    "        featureidkey=\"properties.zone\",\n",
    "        locations = t['dropoff_zone'],\n",
    "        z = t['avg(count)'],\n",
    "        zauto=False,\n",
    "        colorscale='ylorrd',\n",
    "        marker={'opacity':0.8,'line_width':0.5},\n",
    "        hovertext = t['dropoff_zone'],\n",
    "        hoverinfo='text + z',\n",
    "        showscale=True\n",
    "    )\n",
    "    \n",
    "    datas.append(data)\n",
    "    \n",
    "    visibles = [False]*24\n",
    "    visibles[hour] = True\n",
    "    \n",
    "    step = {\n",
    "        \"args\": [{\"visible\":visibles}],\n",
    "        \"label\": hour+1,\n",
    "        \"method\": \"restyle\"\n",
    "    }\n",
    "    \n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [{\"x\":0,\"y\":0.15,\"pad\":{\"t\":24},\"steps\":steps,\"active\":1,\"currentvalue\":{\"prefix\":\"hour_dropoff\"}}]\n",
    "    \n",
    "\n",
    "avg_dropoffs_fig = go.Figure(\n",
    "    data = datas\n",
    ")\n",
    "\n",
    "avg_dropoffs_fig.update_layout(\n",
    "    sliders = sliders,\n",
    "    mapbox = {'accesstoken':token,'center':{'lon':-73.965691,'lat':40.97},'zoom':5},margin = {'l':1,'r':1,'t':1,'b':1})\n",
    "avg_dropoffs_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### average ratio of tip over total fare amount for pickups in the area at given hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:50:32.195979Z",
     "start_time": "2022-03-13T19:50:11.890924Z"
    }
   },
   "outputs": [],
   "source": [
    "sumFare2018 = interactive2018\\\n",
    "            .groupBy('date', 'hour','PULocationID')\\\n",
    "            .agg(fn.round(fn.sum(col('total_amount')),2).alias('sum_fare'), \n",
    "                fn.round(fn.sum(col('tip_amount')),2).alias('sum_tip'))\\\n",
    "            .orderBy('date','hour')\n",
    "\n",
    "sumFare2018 = sumFare2018\\\n",
    "            .groupBy('hour','PULocationID')\\\n",
    "            .agg(fn.round(fn.avg(col('sum_fare')),2).alias('avg_fare'), \n",
    "                 fn.round(fn.avg(col('sum_tip')),2).alias('avg_tip'))\\\n",
    "            .orderBy('hour')\n",
    "                \n",
    "ratio2018 = sumFare2018\\\n",
    "            .withColumn('ratio',fn.round(fn.col('avg_tip')/fn.col('avg_fare'),2))\\\n",
    "            .cache()\n",
    "ratio2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:50:37.865407Z",
     "start_time": "2022-03-13T19:50:36.683425Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_ratio2018 = toPandas(ratio2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:50:39.746515Z",
     "start_time": "2022-03-13T19:50:39.721147Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_ratio2018.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T09:45:07.782759Z",
     "start_time": "2022-03-13T09:42:37.304378Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(\n",
    "    pd_ratio2018, \n",
    "    geojson=taxi_zone, locations='PULocationID', color='ratio',\n",
    "    featureidkey=\"properties.location_id\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    zoom=9.5, center = {\"lat\": 40.74, \"lon\": -73.96},\n",
    "    animation_frame=\"hour\",\n",
    "    opacity=0.5,\n",
    "    labels={'ratio':'average ratio of tip over total fare'}\n",
    "    title = {'average ratio of tip over total fare amount for pickups in 2018'}\n",
    ")\n",
    "\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015\n",
    "ratio_pickup_fare = df2015_add.groupby('pickup_zone','hour_pickup').agg(sum(df2015_add['tip_amount']),sum(df2015_add['total_amount']))\n",
    "ratio_pickup_fare.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratio_pickup_fare = ratio_pickup_fare.withColumn('ratio',ratio_pickup_fare['sum(tip_amount)']/ratio_pickup_fare['sum(total_amount)']).select('pickup_zone','hour_pickup','ratio')\n",
    "avg_ratio_pickup_fare.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratio_pickup_fare = avg_ratio_pickup_fare.toPandas()\n",
    "\n",
    "steps = []\n",
    "datas = []\n",
    "\n",
    "for hour in range(24):\n",
    "    t = avg_ratio_pickup_fare[avg_ratio_pickup_fare['hour_pickup'] == hour]\n",
    "    data = go.Choroplethmapbox(\n",
    "        geojson = taxi_zone,\n",
    "        featureidkey=\"properties.zone\",\n",
    "        locations = t['pickup_zone'],\n",
    "        z = t['ratio'],\n",
    "        zauto=False,\n",
    "        colorscale='ylorrd',\n",
    "        marker={'opacity':0.8,'line_width':0.5},\n",
    "        hovertext = t['pickup_zone'],\n",
    "        hoverinfo='text + z',\n",
    "        showscale=True\n",
    "    )\n",
    "    \n",
    "    datas.append(data)\n",
    "    \n",
    "    visibles = [False]*24\n",
    "    visibles[hour] = True\n",
    "    \n",
    "    step = {\n",
    "        \"args\": [{\"visible\":visibles}],\n",
    "        \"label\": hour+1,\n",
    "        \"method\": \"restyle\"\n",
    "    }\n",
    "    \n",
    "    steps.append(step)\n",
    "\n",
    "\n",
    "sliders = [{\"x\":0,\"y\":0.15,\"pad\":{\"t\":24},\"steps\":steps,\"active\":1,\"currentvalue\":{\"prefix\":\"hour_pickup\"}}]\n",
    "    \n",
    "\n",
    "avg_pickup_fig = go.Figure(\n",
    "    data = datas\n",
    ")\n",
    "\n",
    "avg_pickup_fig.update_layout(\n",
    "    sliders = sliders,\n",
    "    mapbox = {'accesstoken':token,'center':{'lon':-73.965691,'lat':40.97},'zoom':5},margin = {'l':1,'r':1,'t':1,'b':1})\n",
    "avg_pickup_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T19:50:56.887343Z",
     "start_time": "2022-03-13T19:50:48.887197Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
